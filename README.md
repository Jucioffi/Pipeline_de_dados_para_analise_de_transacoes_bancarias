# Pipeline de Dados para An√°lise de Transa√ß√µes Banc√°rias

Este reposit√≥rio cont√©m o desenvolvimento de um *Minimum Viable Product (MVP)* de engenharia de dados voltado √† an√°lise de transa√ß√µes banc√°rias, realizado como parte da p√≥s-gradua√ß√£o em Ci√™ncia de Dados na PUC-Rio.

O trabalho tem como foco a constru√ß√£o de um pipeline de dados em nuvem, contemplando as etapas de busca, coleta, modelagem, carga e an√°lise dos dados, utilizando a plataforma Databricks e o formato Delta Lake, seguindo a arquitetura Bronze‚ÄìSilver‚ÄìGold.

---

## Objetivo do Projeto

O objetivo do projeto √© desenvolver um pipeline anal√≠tico completo e reprodut√≠vel para dados transacionais banc√°rios, permitindo a explora√ß√£o de padr√µes de comportamento financeiro e operacional.

A partir do dataset utilizado, o trabalho busca responder perguntas como:
- Como os valores das transa√ß√µes se distribuem ao longo do tempo?
- Existem padr√µes distintos de acordo com o tipo de transa√ß√£o e o canal utilizado?
- Quais vari√°veis operacionais est√£o mais associadas ao volume financeiro movimentado?
- H√° ind√≠cios de risco operacional observ√°veis a partir das caracter√≠sticas das transa√ß√µes?

Mais do que responder a essas perguntas, o projeto busca demonstrar a aplica√ß√£o de boas pr√°ticas de engenharia de dados em ambiente de nuvem.

---

## Conjunto de Dados

O dataset utilizado √© p√∫blico e est√° dispon√≠vel neste reposit√≥rio:

- **Arquivo:** `bank_transactions_data_2.csv`
- **Formato:** CSV
- **Conte√∫do:** registros de transa√ß√µes banc√°rias com atributos financeiros, temporais, operacionais e demogr√°ficos.

O conjunto de dados n√£o cont√©m informa√ß√µes sens√≠veis ou identific√°veis de indiv√≠duos reais e √© utilizado exclusivamente para fins educacionais.

---

## Arquitetura do Pipeline

O pipeline foi estruturado seguindo a arquitetura **Bronze‚ÄìSilver‚ÄìGold**:

### üîπ Bronze (Raw)
- Ingest√£o dos dados brutos exatamente como fornecidos pela fonte.
- Persist√™ncia em formato Delta Lake.
- Preserva√ß√£o da integridade e rastreabilidade dos dados.

### üîπ Silver (Curated)
- Padroniza√ß√£o de tipos de dados.
- Tratamento de datas, valores num√©ricos e colunas categ√≥ricas.
- Cria√ß√£o de atributos derivados.
- Prepara√ß√£o dos dados para an√°lise e modelagem dimensional.

### üîπ Gold (Analytics)
- Modelagem dimensional em esquema estrela.
- Estrutura√ß√£o de tabelas fato e dimens√µes.
- Base pronta para an√°lises anal√≠ticas e explora√ß√£o de indicadores.

---

## Modelagem de Dados

A modelagem foi realizada no formato **Esquema Estrela**, composta por:
- Tabela fato de transa√ß√µes;
- Dimens√µes de tempo, tipo de transa√ß√£o, canal, categoria e usu√°rio.

Al√©m da modelagem, foi constru√≠do um **Cat√°logo de Dados**, contendo:
- descri√ß√£o dos atributos;
- tipos e dom√≠nios esperados;
- linhagem dos dados desde a fonte at√© as camadas anal√≠ticas.

---

## An√°lises Realizadas

A etapa de an√°lise foi dividida em dois grandes blocos:

### Qualidade de Dados
- Avalia√ß√£o de valores nulos, tipos inconsistentes e faixas inv√°lidas.
- Discuss√£o dos impactos desses problemas nas an√°lises.
- Valida√ß√£o da consist√™ncia dos dados ap√≥s o tratamento.

### Solu√ß√£o do Problema
- An√°lises explorat√≥rias orientadas √†s perguntas de neg√≥cio.
- Interpreta√ß√£o dos resultados √† luz do contexto transacional.
- Discuss√£o integrada dos achados, destacando o papel de vari√°veis operacionais e temporais.

---

## Desafios

Durante o desenvolvimento do projeto, foram identificadas restri√ß√µes t√©cnicas no **Databricks Community Edition**, tais como:
- Limita√ß√£o na leitura direta de arquivos via HTTP/HTTPS usando `spark.read.csv()`;
- Bloqueio de acesso ao sistema de arquivos local do driver;
- Desativa√ß√£o do DBFS p√∫blico (`/FileStore`).

Diante dessas restri√ß√µes, foi adotada uma solu√ß√£o t√©cnica baseada no uso de **Volumes (Unity Catalog)** como √°rea de aterrissagem (*landing zone*) dos dados. Essa abordagem permitiu manter o armazenamento em nuvem, garantir governan√ßa e assegurar a execu√ß√£o consistente do pipeline, transformando uma limita√ß√£o da plataforma em uma decis√£o arquitetural consciente.

---

## Tecnologias Utilizadas:

- Databricks Community Edition
- Apache Spark
- Delta Lake
- Python (PySpark)
- GitHub

---

## Considera√ß√µes Finais e Autoavalia√ß√£o

O MVP desenvolvido atende aos requisitos propostos, contemplando de forma consistente todas as etapas exigidas para a constru√ß√£o de um pipeline de dados em nuvem: busca, coleta, modelagem, carga e an√°lise. A solu√ß√£o implementada permitiu transformar um conjunto de dados bruto em uma base anal√≠tica estruturada, documentada e alinhada √†s boas pr√°ticas de engenharia de dados, utilizando a arquitetura Bronze‚ÄìSilver‚ÄìGold.

Durante o desenvolvimento, um dos principais desafios enfrentados esteve relacionado √†s **limita√ß√µes t√©cnicas do Databricks Community Edition**, especialmente no que diz respeito √† ingest√£o de dados diretamente a partir de fontes externas em nuvem. Foram realizadas tentativas de importa√ß√£o do dataset a partir de um **bucket no Amazon S3** e tamb√©m via **Google Cloud Storage**, por√©m ambas as abordagens apresentaram restri√ß√µes na vers√£o Community da plataforma, impedindo a leitura direta dos dados por meio de URLs ou conectores nativos sem o uso de credenciais ou recursos pagos.

Essas limita√ß√µes impactaram a estrat√©gia inicial de ingest√£o, exigindo um maior aprofundamento na compreens√£o do ambiente do Databricks e levando √† ado√ß√£o de uma solu√ß√£o alternativa baseada no uso de **Volumes (Unity Catalog)** como √°rea de aterrissagem dos dados. Essa decis√£o permitiu manter o armazenamento em nuvem, garantir governan√ßa, rastreabilidade e reprodutibilidade do pipeline, transformando uma restri√ß√£o da plataforma em uma escolha arquitetural consciente.

Outro aspecto desafiador foi equilibrar o **rigor t√©cnico da implementa√ß√£o** com a **clareza documental exigida pelo trabalho**, uma vez que o projeto demandou n√£o apenas a execu√ß√£o do pipeline, mas tamb√©m a explicita√ß√£o das decis√µes de modelagem, dos processos de transforma√ß√£o e das an√°lises realizadas. Esse esfor√ßo contribuiu significativamente para o amadurecimento da vis√£o pr√°tica sobre engenharia de dados aplicada.

Apesar das dificuldades encontradas, os objetivos centrais do trabalho foram atingidos, e as perguntas de neg√≥cio definidas inicialmente puderam ser exploradas de forma coerente a partir das an√°lises desenvolvidas. Como extens√µes futuras, o projeto poderia ser enriquecido com a integra√ß√£o de ferramentas de visualiza√ß√£o (como Power BI ou Databricks SQL), automa√ß√£o do pipeline e aplica√ß√£o de modelos anal√≠ticos ou preditivos sobre a camada Gold.

De forma geral, o desenvolvimento deste MVP consolidou o entendimento pr√°tico sobre pipelines de dados em nuvem e evidenciou a import√¢ncia de adaptar solu√ß√µes t√©cnicas √†s restri√ß√µes reais das plataformas utilizadas, refor√ßando a relev√¢ncia de decis√µes arquiteturais bem fundamentadas no contexto de projetos de dados.


---

## Autor

**J√∫lio Cioffi**  
Projeto desenvolvido como parte da p√≥s-gradua√ß√£o em Ci√™ncia de Dados ‚Äì PUC-Rio.

